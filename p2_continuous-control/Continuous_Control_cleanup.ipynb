{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import utils\n",
    "import TD3\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='/home/corey/reinforcement-learning/udacity/deep-reinforcement-learning/p2_continuous-control/Reacher_Linux/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "timesteps = 0\n",
    "while True:\n",
    "    timesteps += 1\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD3 with reacher env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "policy =\"TD3\"                 # Policy name (TD3, DDPG or OurDDPG)\n",
    "seed = 0               # Sets Gym, PyTorch and Numpy seeds\n",
    "start_timesteps = 25e3# Time steps initial random policy is used\n",
    "max_timesteps = 329_329\n",
    "# max_timesteps = 50_000\n",
    "eval_freq = 5e3      # How often (time steps) we evaluate\n",
    "# max_timesteps = int(1e6)   # Max time steps to run environment\n",
    "expl_noise = 0.1              # Std of Gaussian exploration noise\n",
    "batch_size = 256      # Batch size for both actor and critic\n",
    "discount = 0.99                 # Discount factor\n",
    "tau = 0.005                     # Target network update rate\n",
    "policy_noise = 0.2              # Noise added to target policy during critic update\n",
    "noise_clip = 0.5                # Range to clip target policy noise\n",
    "policy_freq = 2       # Frequency of delayed policy updates\n",
    "save_model = True        # Save model and optimizer parameters\n",
    "load_model = \"\"                # Model load file name, \"\" doesn't load, \"default\" uses file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcoreymorris\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/corey/reinforcement-learning/udacity/deep-reinforcement-learning/p2_continuous-control/wandb/run-20230308_133849-htl12wdb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/coreymorris/Continuous-Control/runs/htl12wdb' target=\"_blank\">happy-shadow-19</a></strong> to <a href='https://wandb.ai/coreymorris/Continuous-Control' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/coreymorris/Continuous-Control' target=\"_blank\">https://wandb.ai/coreymorris/Continuous-Control</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/coreymorris/Continuous-Control/runs/htl12wdb' target=\"_blank\">https://wandb.ai/coreymorris/Continuous-Control/runs/htl12wdb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "wandb.init(project=\"Continuous-Control\")\n",
    "\n",
    "wandb.config = {\n",
    "    \"policy\": policy,\n",
    "    \"seed\": seed, \n",
    "    #TODO add the rest\n",
    "}\n",
    "\n",
    "if not os.path.exists(\"./results\"):\n",
    "    os.makedirs(\"./results\")\n",
    "\n",
    "if save_model and not os.path.exists(\"./models\"):\n",
    "    os.makedirs(\"./models\")\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "state_dim = state_size\n",
    "action_dim = action_size\n",
    "max_action = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"state_dim\": state_dim,\n",
    "    \"action_dim\": action_dim,\n",
    "    \"max_action\": max_action,\n",
    "    \"discount\": discount,\n",
    "    \"tau\": tau,\n",
    "}\n",
    "\n",
    "# Initialize policy\n",
    "if policy == \"TD3\":\n",
    "    # Target policy smoothing is scaled wrt the action scale\n",
    "    kwargs[\"policy_noise\"] = policy_noise * max_action\n",
    "    kwargs[\"noise_clip\"] = noise_clip * max_action\n",
    "    kwargs[\"policy_freq\"] = policy_freq\n",
    "    policy = TD3.TD3(**kwargs)\n",
    "\n",
    "if load_model != \"\":\n",
    "    policy_file = file_name if load_model == \"default\" else load_model\n",
    "    policy.load(f\"./models/{policy_file}\")\n",
    "\n",
    "replay_buffer = utils.ReplayBuffer(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Evaluate untrained policy\n",
    "# evaluations = [eval_policy(policy, env_name, seed)]\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name] \n",
    "state = env_info.vector_observations[0]\n",
    "done = env_info.local_done[0]\n",
    "  \n",
    "episode_reward = 0\n",
    "episode_timesteps = 0\n",
    "episode_num = 0\n",
    "\n",
    "wandb.watch(policy.actor, log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T: 1001 Episode Num: 1 Episode T: 1001 Reward: 0.000\n",
      "Total T: 2002 Episode Num: 2 Episode T: 1001 Reward: 0.000\n",
      "Total T: 3003 Episode Num: 3 Episode T: 1001 Reward: 0.000\n",
      "Total T: 4004 Episode Num: 4 Episode T: 1001 Reward: 0.000\n",
      "Total T: 5005 Episode Num: 5 Episode T: 1001 Reward: 0.000\n",
      "Total T: 6006 Episode Num: 6 Episode T: 1001 Reward: 0.000\n",
      "Total T: 7007 Episode Num: 7 Episode T: 1001 Reward: 0.000\n",
      "Total T: 8008 Episode Num: 8 Episode T: 1001 Reward: 0.000\n",
      "Total T: 9009 Episode Num: 9 Episode T: 1001 Reward: 0.000\n",
      "Total T: 10010 Episode Num: 10 Episode T: 1001 Reward: 0.000\n",
      "Total T: 11011 Episode Num: 11 Episode T: 1001 Reward: 0.000\n",
      "Total T: 12012 Episode Num: 12 Episode T: 1001 Reward: 0.000\n",
      "Total T: 13013 Episode Num: 13 Episode T: 1001 Reward: 0.000\n",
      "Total T: 14014 Episode Num: 14 Episode T: 1001 Reward: 0.000\n",
      "Total T: 15015 Episode Num: 15 Episode T: 1001 Reward: 0.000\n",
      "Total T: 16016 Episode Num: 16 Episode T: 1001 Reward: 0.000\n",
      "Total T: 17017 Episode Num: 17 Episode T: 1001 Reward: 0.000\n",
      "Total T: 18018 Episode Num: 18 Episode T: 1001 Reward: 0.000\n",
      "Total T: 19019 Episode Num: 19 Episode T: 1001 Reward: 0.000\n",
      "Total T: 20020 Episode Num: 20 Episode T: 1001 Reward: 0.000\n",
      "Total T: 21021 Episode Num: 21 Episode T: 1001 Reward: 0.000\n",
      "Total T: 22022 Episode Num: 22 Episode T: 1001 Reward: 0.000\n",
      "Total T: 23023 Episode Num: 23 Episode T: 1001 Reward: 0.000\n",
      "Total T: 24024 Episode Num: 24 Episode T: 1001 Reward: 0.000\n",
      "Total T: 25025 Episode Num: 25 Episode T: 1001 Reward: 0.000\n",
      "Total T: 26026 Episode Num: 26 Episode T: 1001 Reward: 1.410\n",
      "Total T: 27027 Episode Num: 27 Episode T: 1001 Reward: 1.510\n",
      "Total T: 28028 Episode Num: 28 Episode T: 1001 Reward: 0.210\n",
      "Total T: 29029 Episode Num: 29 Episode T: 1001 Reward: 1.530\n",
      "Total T: 30030 Episode Num: 30 Episode T: 1001 Reward: 0.560\n",
      "Total T: 31031 Episode Num: 31 Episode T: 1001 Reward: 1.040\n",
      "Total T: 32032 Episode Num: 32 Episode T: 1001 Reward: 0.700\n",
      "Total T: 33033 Episode Num: 33 Episode T: 1001 Reward: 0.000\n",
      "Total T: 34034 Episode Num: 34 Episode T: 1001 Reward: 0.760\n",
      "Total T: 35035 Episode Num: 35 Episode T: 1001 Reward: 1.180\n",
      "Total T: 36036 Episode Num: 36 Episode T: 1001 Reward: 0.330\n",
      "Total T: 37037 Episode Num: 37 Episode T: 1001 Reward: 0.540\n",
      "Total T: 38038 Episode Num: 38 Episode T: 1001 Reward: 0.740\n",
      "Total T: 39039 Episode Num: 39 Episode T: 1001 Reward: 1.330\n",
      "Total T: 40040 Episode Num: 40 Episode T: 1001 Reward: 2.470\n",
      "Total T: 41041 Episode Num: 41 Episode T: 1001 Reward: 2.830\n",
      "Total T: 42042 Episode Num: 42 Episode T: 1001 Reward: 2.600\n",
      "Total T: 43043 Episode Num: 43 Episode T: 1001 Reward: 1.700\n",
      "Total T: 44044 Episode Num: 44 Episode T: 1001 Reward: 1.960\n",
      "Total T: 45045 Episode Num: 45 Episode T: 1001 Reward: 1.830\n",
      "Total T: 46046 Episode Num: 46 Episode T: 1001 Reward: 4.410\n",
      "Total T: 47047 Episode Num: 47 Episode T: 1001 Reward: 2.940\n",
      "Total T: 48048 Episode Num: 48 Episode T: 1001 Reward: 1.040\n",
      "Total T: 49049 Episode Num: 49 Episode T: 1001 Reward: 2.730\n",
      "Total T: 50050 Episode Num: 50 Episode T: 1001 Reward: 0.520\n",
      "Total T: 51051 Episode Num: 51 Episode T: 1001 Reward: 4.430\n",
      "Total T: 52052 Episode Num: 52 Episode T: 1001 Reward: 5.930\n",
      "Total T: 53053 Episode Num: 53 Episode T: 1001 Reward: 4.660\n",
      "Total T: 54054 Episode Num: 54 Episode T: 1001 Reward: 2.870\n",
      "Total T: 55055 Episode Num: 55 Episode T: 1001 Reward: 4.130\n",
      "Total T: 56056 Episode Num: 56 Episode T: 1001 Reward: 4.790\n",
      "Total T: 57057 Episode Num: 57 Episode T: 1001 Reward: 5.720\n",
      "Total T: 58058 Episode Num: 58 Episode T: 1001 Reward: 2.560\n",
      "Total T: 59059 Episode Num: 59 Episode T: 1001 Reward: 7.390\n",
      "Total T: 60060 Episode Num: 60 Episode T: 1001 Reward: 5.520\n",
      "Total T: 61061 Episode Num: 61 Episode T: 1001 Reward: 5.740\n",
      "Total T: 62062 Episode Num: 62 Episode T: 1001 Reward: 3.880\n",
      "Total T: 63063 Episode Num: 63 Episode T: 1001 Reward: 1.750\n",
      "Total T: 64064 Episode Num: 64 Episode T: 1001 Reward: 3.770\n",
      "Total T: 65065 Episode Num: 65 Episode T: 1001 Reward: 5.960\n",
      "Total T: 66066 Episode Num: 66 Episode T: 1001 Reward: 4.100\n",
      "Total T: 67067 Episode Num: 67 Episode T: 1001 Reward: 5.410\n",
      "Total T: 68068 Episode Num: 68 Episode T: 1001 Reward: 11.480\n",
      "Total T: 69069 Episode Num: 69 Episode T: 1001 Reward: 9.730\n",
      "Total T: 70070 Episode Num: 70 Episode T: 1001 Reward: 12.200\n",
      "Total T: 71071 Episode Num: 71 Episode T: 1001 Reward: 12.090\n",
      "Total T: 72072 Episode Num: 72 Episode T: 1001 Reward: 12.660\n",
      "Total T: 73073 Episode Num: 73 Episode T: 1001 Reward: 9.390\n",
      "Total T: 74074 Episode Num: 74 Episode T: 1001 Reward: 6.020\n",
      "Total T: 75075 Episode Num: 75 Episode T: 1001 Reward: 16.390\n",
      "Total T: 76076 Episode Num: 76 Episode T: 1001 Reward: 11.310\n",
      "Total T: 77077 Episode Num: 77 Episode T: 1001 Reward: 10.690\n",
      "Total T: 78078 Episode Num: 78 Episode T: 1001 Reward: 7.330\n",
      "Total T: 79079 Episode Num: 79 Episode T: 1001 Reward: 17.920\n",
      "Total T: 80080 Episode Num: 80 Episode T: 1001 Reward: 8.710\n",
      "Total T: 81081 Episode Num: 81 Episode T: 1001 Reward: 21.350\n",
      "Total T: 82082 Episode Num: 82 Episode T: 1001 Reward: 19.950\n",
      "Total T: 83083 Episode Num: 83 Episode T: 1001 Reward: 13.190\n",
      "Total T: 84084 Episode Num: 84 Episode T: 1001 Reward: 15.650\n",
      "Total T: 85085 Episode Num: 85 Episode T: 1001 Reward: 23.230\n",
      "Total T: 86086 Episode Num: 86 Episode T: 1001 Reward: 20.640\n",
      "Total T: 87087 Episode Num: 87 Episode T: 1001 Reward: 14.630\n",
      "Total T: 88088 Episode Num: 88 Episode T: 1001 Reward: 18.840\n",
      "Total T: 89089 Episode Num: 89 Episode T: 1001 Reward: 12.930\n",
      "Total T: 90090 Episode Num: 90 Episode T: 1001 Reward: 16.700\n",
      "Total T: 91091 Episode Num: 91 Episode T: 1001 Reward: 11.370\n",
      "Total T: 92092 Episode Num: 92 Episode T: 1001 Reward: 19.110\n",
      "Total T: 93093 Episode Num: 93 Episode T: 1001 Reward: 9.870\n",
      "Total T: 94094 Episode Num: 94 Episode T: 1001 Reward: 29.590\n",
      "Total T: 95095 Episode Num: 95 Episode T: 1001 Reward: 15.970\n",
      "Total T: 96096 Episode Num: 96 Episode T: 1001 Reward: 12.900\n",
      "Total T: 97097 Episode Num: 97 Episode T: 1001 Reward: 30.060\n",
      "Total T: 98098 Episode Num: 98 Episode T: 1001 Reward: 14.700\n",
      "Total T: 99099 Episode Num: 99 Episode T: 1001 Reward: 29.360\n",
      "Total T: 100100 Episode Num: 100 Episode T: 1001 Reward: 18.550\n",
      "Episode 100\tAverage Score: 6.30\n",
      "Total T: 101101 Episode Num: 101 Episode T: 1001 Reward: 18.920\n",
      "Total T: 102102 Episode Num: 102 Episode T: 1001 Reward: 25.090\n",
      "Total T: 103103 Episode Num: 103 Episode T: 1001 Reward: 18.000\n",
      "Total T: 104104 Episode Num: 104 Episode T: 1001 Reward: 20.770\n",
      "Total T: 105105 Episode Num: 105 Episode T: 1001 Reward: 23.280\n",
      "Total T: 106106 Episode Num: 106 Episode T: 1001 Reward: 19.630\n",
      "Total T: 107107 Episode Num: 107 Episode T: 1001 Reward: 30.140\n",
      "Total T: 108108 Episode Num: 108 Episode T: 1001 Reward: 37.880\n",
      "Total T: 109109 Episode Num: 109 Episode T: 1001 Reward: 32.500\n",
      "Total T: 110110 Episode Num: 110 Episode T: 1001 Reward: 21.910\n",
      "Total T: 111111 Episode Num: 111 Episode T: 1001 Reward: 21.590\n",
      "Total T: 112112 Episode Num: 112 Episode T: 1001 Reward: 24.010\n",
      "Total T: 113113 Episode Num: 113 Episode T: 1001 Reward: 31.810\n",
      "Total T: 114114 Episode Num: 114 Episode T: 1001 Reward: 27.360\n",
      "Total T: 115115 Episode Num: 115 Episode T: 1001 Reward: 26.480\n",
      "Total T: 116116 Episode Num: 116 Episode T: 1001 Reward: 34.300\n",
      "Total T: 117117 Episode Num: 117 Episode T: 1001 Reward: 20.720\n",
      "Total T: 118118 Episode Num: 118 Episode T: 1001 Reward: 29.530\n",
      "Total T: 119119 Episode Num: 119 Episode T: 1001 Reward: 36.190\n",
      "Total T: 120120 Episode Num: 120 Episode T: 1001 Reward: 34.190\n",
      "Total T: 121121 Episode Num: 121 Episode T: 1001 Reward: 33.690\n",
      "Total T: 122122 Episode Num: 122 Episode T: 1001 Reward: 38.020\n",
      "Total T: 123123 Episode Num: 123 Episode T: 1001 Reward: 29.980\n",
      "Total T: 124124 Episode Num: 124 Episode T: 1001 Reward: 33.750\n",
      "Total T: 125125 Episode Num: 125 Episode T: 1001 Reward: 28.640\n",
      "Total T: 126126 Episode Num: 126 Episode T: 1001 Reward: 27.920\n",
      "Total T: 127127 Episode Num: 127 Episode T: 1001 Reward: 33.590\n",
      "Total T: 128128 Episode Num: 128 Episode T: 1001 Reward: 36.380\n",
      "Total T: 129129 Episode Num: 129 Episode T: 1001 Reward: 17.290\n",
      "Total T: 130130 Episode Num: 130 Episode T: 1001 Reward: 31.040\n",
      "Total T: 131131 Episode Num: 131 Episode T: 1001 Reward: 22.620\n",
      "Total T: 132132 Episode Num: 132 Episode T: 1001 Reward: 34.110\n",
      "Total T: 133133 Episode Num: 133 Episode T: 1001 Reward: 31.730\n",
      "Total T: 134134 Episode Num: 134 Episode T: 1001 Reward: 32.080\n",
      "Total T: 135135 Episode Num: 135 Episode T: 1001 Reward: 33.340\n",
      "Total T: 136136 Episode Num: 136 Episode T: 1001 Reward: 33.400\n",
      "Total T: 137137 Episode Num: 137 Episode T: 1001 Reward: 35.690\n",
      "Total T: 138138 Episode Num: 138 Episode T: 1001 Reward: 36.320\n",
      "Total T: 139139 Episode Num: 139 Episode T: 1001 Reward: 35.810\n",
      "Total T: 140140 Episode Num: 140 Episode T: 1001 Reward: 37.920\n",
      "Total T: 141141 Episode Num: 141 Episode T: 1001 Reward: 32.270\n",
      "Total T: 142142 Episode Num: 142 Episode T: 1001 Reward: 36.850\n",
      "Total T: 143143 Episode Num: 143 Episode T: 1001 Reward: 34.960\n",
      "Total T: 144144 Episode Num: 144 Episode T: 1001 Reward: 35.230\n",
      "Total T: 145145 Episode Num: 145 Episode T: 1001 Reward: 30.340\n",
      "Total T: 146146 Episode Num: 146 Episode T: 1001 Reward: 37.940\n",
      "Total T: 147147 Episode Num: 147 Episode T: 1001 Reward: 30.460\n",
      "Total T: 148148 Episode Num: 148 Episode T: 1001 Reward: 27.270\n",
      "Total T: 149149 Episode Num: 149 Episode T: 1001 Reward: 36.560\n",
      "Total T: 150150 Episode Num: 150 Episode T: 1001 Reward: 36.130\n",
      "Total T: 151151 Episode Num: 151 Episode T: 1001 Reward: 34.170\n",
      "Total T: 152152 Episode Num: 152 Episode T: 1001 Reward: 34.110\n",
      "Total T: 153153 Episode Num: 153 Episode T: 1001 Reward: 39.180\n",
      "Total T: 154154 Episode Num: 154 Episode T: 1001 Reward: 37.870\n",
      "Total T: 155155 Episode Num: 155 Episode T: 1001 Reward: 25.520\n",
      "Total T: 156156 Episode Num: 156 Episode T: 1001 Reward: 37.620\n",
      "Total T: 157157 Episode Num: 157 Episode T: 1001 Reward: 36.180\n",
      "Total T: 158158 Episode Num: 158 Episode T: 1001 Reward: 32.350\n",
      "Total T: 159159 Episode Num: 159 Episode T: 1001 Reward: 35.670\n",
      "Total T: 160160 Episode Num: 160 Episode T: 1001 Reward: 36.970\n",
      "Total T: 161161 Episode Num: 161 Episode T: 1001 Reward: 39.270\n",
      "Total T: 162162 Episode Num: 162 Episode T: 1001 Reward: 37.820\n",
      "Total T: 163163 Episode Num: 163 Episode T: 1001 Reward: 31.900\n",
      "Total T: 164164 Episode Num: 164 Episode T: 1001 Reward: 38.420\n",
      "Total T: 165165 Episode Num: 165 Episode T: 1001 Reward: 37.860\n",
      "Total T: 166166 Episode Num: 166 Episode T: 1001 Reward: 39.460\n",
      "Total T: 167167 Episode Num: 167 Episode T: 1001 Reward: 38.300\n",
      "Total T: 168168 Episode Num: 168 Episode T: 1001 Reward: 36.120\n",
      "Total T: 169169 Episode Num: 169 Episode T: 1001 Reward: 34.680\n",
      "Total T: 170170 Episode Num: 170 Episode T: 1001 Reward: 38.970\n",
      "Total T: 171171 Episode Num: 171 Episode T: 1001 Reward: 35.900\n",
      "Total T: 172172 Episode Num: 172 Episode T: 1001 Reward: 38.370\n",
      "Total T: 173173 Episode Num: 173 Episode T: 1001 Reward: 36.430\n",
      "Total T: 174174 Episode Num: 174 Episode T: 1001 Reward: 35.530\n",
      "Total T: 175175 Episode Num: 175 Episode T: 1001 Reward: 34.570\n",
      "Total T: 176176 Episode Num: 176 Episode T: 1001 Reward: 37.850\n",
      "Total T: 177177 Episode Num: 177 Episode T: 1001 Reward: 34.080\n",
      "Total T: 178178 Episode Num: 178 Episode T: 1001 Reward: 33.610\n",
      "Total T: 179179 Episode Num: 179 Episode T: 1001 Reward: 35.190\n",
      "Total T: 180180 Episode Num: 180 Episode T: 1001 Reward: 39.030\n",
      "Total T: 181181 Episode Num: 181 Episode T: 1001 Reward: 39.330\n",
      "Total T: 182182 Episode Num: 182 Episode T: 1001 Reward: 38.960\n",
      "Total T: 183183 Episode Num: 183 Episode T: 1001 Reward: 37.910\n",
      "Total T: 184184 Episode Num: 184 Episode T: 1001 Reward: 29.360\n",
      "Total T: 185185 Episode Num: 185 Episode T: 1001 Reward: 38.720\n",
      "Total T: 186186 Episode Num: 186 Episode T: 1001 Reward: 38.480\n",
      "Total T: 187187 Episode Num: 187 Episode T: 1001 Reward: 38.150\n",
      "Total T: 188188 Episode Num: 188 Episode T: 1001 Reward: 33.440\n",
      "Total T: 189189 Episode Num: 189 Episode T: 1001 Reward: 36.720\n",
      "Total T: 190190 Episode Num: 190 Episode T: 1001 Reward: 34.420\n",
      "Total T: 191191 Episode Num: 191 Episode T: 1001 Reward: 37.140\n",
      "Total T: 192192 Episode Num: 192 Episode T: 1001 Reward: 38.640\n",
      "Total T: 193193 Episode Num: 193 Episode T: 1001 Reward: 37.340\n",
      "Total T: 194194 Episode Num: 194 Episode T: 1001 Reward: 38.480\n",
      "Total T: 195195 Episode Num: 195 Episode T: 1001 Reward: 34.130\n",
      "Total T: 196196 Episode Num: 196 Episode T: 1001 Reward: 39.290\n",
      "Total T: 197197 Episode Num: 197 Episode T: 1001 Reward: 28.680\n",
      "Total T: 198198 Episode Num: 198 Episode T: 1001 Reward: 34.730\n",
      "Total T: 199199 Episode Num: 199 Episode T: 1001 Reward: 36.840\n",
      "Total T: 200200 Episode Num: 200 Episode T: 1001 Reward: 39.020\n",
      "Episode 200\tAverage Score: 33.28\n",
      "Total T: 201201 Episode Num: 201 Episode T: 1001 Reward: 37.540\n",
      "Total T: 202202 Episode Num: 202 Episode T: 1001 Reward: 37.010\n",
      "Total T: 203203 Episode Num: 203 Episode T: 1001 Reward: 39.100\n",
      "Total T: 204204 Episode Num: 204 Episode T: 1001 Reward: 37.280\n",
      "Total T: 205205 Episode Num: 205 Episode T: 1001 Reward: 38.990\n",
      "Total T: 206206 Episode Num: 206 Episode T: 1001 Reward: 36.400\n",
      "Total T: 207207 Episode Num: 207 Episode T: 1001 Reward: 28.570\n",
      "Total T: 208208 Episode Num: 208 Episode T: 1001 Reward: 34.710\n",
      "Total T: 209209 Episode Num: 209 Episode T: 1001 Reward: 36.020\n",
      "Total T: 210210 Episode Num: 210 Episode T: 1001 Reward: 33.350\n",
      "Total T: 211211 Episode Num: 211 Episode T: 1001 Reward: 38.470\n",
      "Total T: 212212 Episode Num: 212 Episode T: 1001 Reward: 35.180\n",
      "Total T: 213213 Episode Num: 213 Episode T: 1001 Reward: 39.660\n",
      "Total T: 214214 Episode Num: 214 Episode T: 1001 Reward: 35.730\n",
      "Total T: 215215 Episode Num: 215 Episode T: 1001 Reward: 36.650\n",
      "Total T: 216216 Episode Num: 216 Episode T: 1001 Reward: 38.740\n",
      "Total T: 217217 Episode Num: 217 Episode T: 1001 Reward: 37.330\n",
      "Total T: 218218 Episode Num: 218 Episode T: 1001 Reward: 39.000\n",
      "Total T: 219219 Episode Num: 219 Episode T: 1001 Reward: 39.220\n",
      "Total T: 220220 Episode Num: 220 Episode T: 1001 Reward: 38.310\n",
      "Total T: 221221 Episode Num: 221 Episode T: 1001 Reward: 37.950\n",
      "Total T: 222222 Episode Num: 222 Episode T: 1001 Reward: 35.760\n",
      "Total T: 223223 Episode Num: 223 Episode T: 1001 Reward: 38.130\n",
      "Total T: 224224 Episode Num: 224 Episode T: 1001 Reward: 37.880\n",
      "Total T: 225225 Episode Num: 225 Episode T: 1001 Reward: 37.690\n",
      "Total T: 226226 Episode Num: 226 Episode T: 1001 Reward: 38.680\n",
      "Total T: 227227 Episode Num: 227 Episode T: 1001 Reward: 31.340\n",
      "Total T: 228228 Episode Num: 228 Episode T: 1001 Reward: 39.150\n",
      "Total T: 229229 Episode Num: 229 Episode T: 1001 Reward: 36.850\n",
      "Total T: 230230 Episode Num: 230 Episode T: 1001 Reward: 37.830\n",
      "Total T: 231231 Episode Num: 231 Episode T: 1001 Reward: 39.040\n",
      "Total T: 232232 Episode Num: 232 Episode T: 1001 Reward: 32.160\n",
      "Total T: 233233 Episode Num: 233 Episode T: 1001 Reward: 36.400\n",
      "Total T: 234234 Episode Num: 234 Episode T: 1001 Reward: 37.780\n",
      "Total T: 235235 Episode Num: 235 Episode T: 1001 Reward: 38.390\n",
      "Total T: 236236 Episode Num: 236 Episode T: 1001 Reward: 38.770\n",
      "Total T: 237237 Episode Num: 237 Episode T: 1001 Reward: 33.450\n",
      "Total T: 238238 Episode Num: 238 Episode T: 1001 Reward: 33.400\n",
      "Total T: 239239 Episode Num: 239 Episode T: 1001 Reward: 39.050\n",
      "Total T: 240240 Episode Num: 240 Episode T: 1001 Reward: 35.020\n",
      "Total T: 241241 Episode Num: 241 Episode T: 1001 Reward: 39.400\n",
      "Total T: 242242 Episode Num: 242 Episode T: 1001 Reward: 39.380\n",
      "Total T: 243243 Episode Num: 243 Episode T: 1001 Reward: 37.700\n",
      "Total T: 244244 Episode Num: 244 Episode T: 1001 Reward: 37.800\n",
      "Total T: 245245 Episode Num: 245 Episode T: 1001 Reward: 39.090\n",
      "Total T: 246246 Episode Num: 246 Episode T: 1001 Reward: 33.840\n",
      "Total T: 247247 Episode Num: 247 Episode T: 1001 Reward: 31.540\n",
      "Total T: 248248 Episode Num: 248 Episode T: 1001 Reward: 37.730\n",
      "Total T: 249249 Episode Num: 249 Episode T: 1001 Reward: 36.110\n",
      "Total T: 250250 Episode Num: 250 Episode T: 1001 Reward: 38.470\n",
      "Total T: 251251 Episode Num: 251 Episode T: 1001 Reward: 37.500\n",
      "Total T: 252252 Episode Num: 252 Episode T: 1001 Reward: 34.610\n",
      "Total T: 253253 Episode Num: 253 Episode T: 1001 Reward: 33.300\n",
      "Total T: 254254 Episode Num: 254 Episode T: 1001 Reward: 37.120\n",
      "Total T: 255255 Episode Num: 255 Episode T: 1001 Reward: 34.030\n",
      "Total T: 256256 Episode Num: 256 Episode T: 1001 Reward: 35.800\n",
      "Total T: 257257 Episode Num: 257 Episode T: 1001 Reward: 35.610\n",
      "Total T: 258258 Episode Num: 258 Episode T: 1001 Reward: 34.090\n",
      "Total T: 259259 Episode Num: 259 Episode T: 1001 Reward: 34.340\n",
      "Total T: 260260 Episode Num: 260 Episode T: 1001 Reward: 34.170\n",
      "Total T: 261261 Episode Num: 261 Episode T: 1001 Reward: 35.750\n",
      "Total T: 262262 Episode Num: 262 Episode T: 1001 Reward: 35.570\n",
      "Total T: 263263 Episode Num: 263 Episode T: 1001 Reward: 39.180\n",
      "Total T: 264264 Episode Num: 264 Episode T: 1001 Reward: 29.880\n",
      "Total T: 265265 Episode Num: 265 Episode T: 1001 Reward: 33.290\n",
      "Total T: 266266 Episode Num: 266 Episode T: 1001 Reward: 38.890\n",
      "Total T: 267267 Episode Num: 267 Episode T: 1001 Reward: 38.150\n",
      "Total T: 268268 Episode Num: 268 Episode T: 1001 Reward: 32.110\n",
      "Total T: 269269 Episode Num: 269 Episode T: 1001 Reward: 39.510\n",
      "Total T: 270270 Episode Num: 270 Episode T: 1001 Reward: 38.390\n",
      "Total T: 271271 Episode Num: 271 Episode T: 1001 Reward: 38.580\n",
      "Total T: 272272 Episode Num: 272 Episode T: 1001 Reward: 38.080\n",
      "Total T: 273273 Episode Num: 273 Episode T: 1001 Reward: 36.360\n",
      "Total T: 274274 Episode Num: 274 Episode T: 1001 Reward: 37.280\n",
      "Total T: 275275 Episode Num: 275 Episode T: 1001 Reward: 36.520\n",
      "Total T: 276276 Episode Num: 276 Episode T: 1001 Reward: 38.910\n",
      "Total T: 277277 Episode Num: 277 Episode T: 1001 Reward: 39.230\n",
      "Total T: 278278 Episode Num: 278 Episode T: 1001 Reward: 35.090\n",
      "Total T: 279279 Episode Num: 279 Episode T: 1001 Reward: 39.180\n",
      "Total T: 280280 Episode Num: 280 Episode T: 1001 Reward: 37.320\n",
      "Total T: 281281 Episode Num: 281 Episode T: 1001 Reward: 39.240\n",
      "Total T: 282282 Episode Num: 282 Episode T: 1001 Reward: 39.220\n",
      "Total T: 283283 Episode Num: 283 Episode T: 1001 Reward: 37.220\n",
      "Total T: 284284 Episode Num: 284 Episode T: 1001 Reward: 34.140\n",
      "Total T: 285285 Episode Num: 285 Episode T: 1001 Reward: 37.100\n",
      "Total T: 286286 Episode Num: 286 Episode T: 1001 Reward: 37.580\n",
      "Total T: 287287 Episode Num: 287 Episode T: 1001 Reward: 37.320\n",
      "Total T: 288288 Episode Num: 288 Episode T: 1001 Reward: 38.750\n",
      "Total T: 289289 Episode Num: 289 Episode T: 1001 Reward: 37.020\n",
      "Total T: 290290 Episode Num: 290 Episode T: 1001 Reward: 39.470\n",
      "Total T: 291291 Episode Num: 291 Episode T: 1001 Reward: 37.410\n",
      "Total T: 292292 Episode Num: 292 Episode T: 1001 Reward: 38.840\n",
      "Total T: 293293 Episode Num: 293 Episode T: 1001 Reward: 39.410\n",
      "Total T: 294294 Episode Num: 294 Episode T: 1001 Reward: 37.950\n",
      "Total T: 295295 Episode Num: 295 Episode T: 1001 Reward: 37.350\n",
      "Total T: 296296 Episode Num: 296 Episode T: 1001 Reward: 38.330\n",
      "Total T: 297297 Episode Num: 297 Episode T: 1001 Reward: 36.220\n",
      "Total T: 298298 Episode Num: 298 Episode T: 1001 Reward: 37.160\n",
      "Total T: 299299 Episode Num: 299 Episode T: 1001 Reward: 38.190\n",
      "Total T: 300300 Episode Num: 300 Episode T: 1001 Reward: 37.430\n",
      "Episode 300\tAverage Score: 36.86\n",
      "Total T: 301301 Episode Num: 301 Episode T: 1001 Reward: 37.540\n",
      "Total T: 302302 Episode Num: 302 Episode T: 1001 Reward: 37.930\n",
      "Total T: 303303 Episode Num: 303 Episode T: 1001 Reward: 38.490\n",
      "Total T: 304304 Episode Num: 304 Episode T: 1001 Reward: 38.260\n",
      "Total T: 305305 Episode Num: 305 Episode T: 1001 Reward: 37.360\n",
      "Total T: 306306 Episode Num: 306 Episode T: 1001 Reward: 39.400\n",
      "Total T: 307307 Episode Num: 307 Episode T: 1001 Reward: 38.410\n",
      "Total T: 308308 Episode Num: 308 Episode T: 1001 Reward: 36.280\n",
      "Total T: 309309 Episode Num: 309 Episode T: 1001 Reward: 37.220\n",
      "Total T: 310310 Episode Num: 310 Episode T: 1001 Reward: 38.520\n",
      "Total T: 311311 Episode Num: 311 Episode T: 1001 Reward: 39.210\n",
      "Total T: 312312 Episode Num: 312 Episode T: 1001 Reward: 37.140\n",
      "Total T: 313313 Episode Num: 313 Episode T: 1001 Reward: 39.650\n",
      "Total T: 314314 Episode Num: 314 Episode T: 1001 Reward: 39.000\n",
      "Total T: 315315 Episode Num: 315 Episode T: 1001 Reward: 24.600\n",
      "Total T: 316316 Episode Num: 316 Episode T: 1001 Reward: 37.090\n",
      "Total T: 317317 Episode Num: 317 Episode T: 1001 Reward: 38.080\n",
      "Total T: 318318 Episode Num: 318 Episode T: 1001 Reward: 36.140\n",
      "Total T: 319319 Episode Num: 319 Episode T: 1001 Reward: 39.250\n",
      "Total T: 320320 Episode Num: 320 Episode T: 1001 Reward: 39.400\n",
      "Total T: 321321 Episode Num: 321 Episode T: 1001 Reward: 35.270\n",
      "Total T: 322322 Episode Num: 322 Episode T: 1001 Reward: 39.190\n",
      "Total T: 323323 Episode Num: 323 Episode T: 1001 Reward: 39.370\n",
      "Total T: 324324 Episode Num: 324 Episode T: 1001 Reward: 34.470\n",
      "Total T: 325325 Episode Num: 325 Episode T: 1001 Reward: 38.020\n",
      "Total T: 326326 Episode Num: 326 Episode T: 1001 Reward: 39.120\n",
      "Total T: 327327 Episode Num: 327 Episode T: 1001 Reward: 32.140\n",
      "Total T: 328328 Episode Num: 328 Episode T: 1001 Reward: 37.790\n",
      "Total T: 329329 Episode Num: 329 Episode T: 1001 Reward: 39.440\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Average Reward over last 100 episodes</td><td>▁▇█</td></tr><tr><td>episode number</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>episode reward</td><td>▁▁▁▁▁▁▂▂▃▃▄▄▄▅▇█▇▇▇█▇█▆██▇████▇▇▇█▇██▇██</td></tr><tr><td>episode timesteps</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total timesteps</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Average Reward over last 100 episodes</td><td>36.8623</td></tr><tr><td>episode number</td><td>329</td></tr><tr><td>episode reward</td><td>39.44</td></tr><tr><td>episode timesteps</td><td>1001</td></tr><tr><td>total timesteps</td><td>329329</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">happy-shadow-19</strong> at: <a href='https://wandb.ai/coreymorris/Continuous-Control/runs/htl12wdb' target=\"_blank\">https://wandb.ai/coreymorris/Continuous-Control/runs/htl12wdb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230308_133849-htl12wdb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print_every = 100\n",
    "scores_deque = deque(maxlen=print_every)\n",
    "scores = []\n",
    "for t in range(int(max_timesteps)):\n",
    "    \n",
    "    episode_timesteps += 1\n",
    "\n",
    "    # Select action randomly or according to policy\n",
    "    if t < start_timesteps:\n",
    "        action = np.random.rand(1, action_size)\n",
    "    else:\n",
    "        action = (\n",
    "            policy.select_action(np.array(state))\n",
    "            + np.random.normal(0, max_action * expl_noise, size=action_dim)\n",
    "        ).clip(-max_action, max_action)\n",
    "\n",
    "    # Perform action\n",
    "    env_info = env.step(action)[brain_name]\n",
    "    next_state = env_info.vector_observations[0]\n",
    "    reward = env_info.rewards[0]\n",
    "    done = env_info.local_done[0]  \n",
    "    done_bool = float(done) if env_info.max_reached[0] else 0\n",
    "\n",
    "    # Store data in replay buffer\n",
    "    replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "\n",
    "    # Train agent after collecting sufficient data\n",
    "    if t >= start_timesteps:\n",
    "        policy.train(replay_buffer, batch_size)\n",
    "\n",
    "    if done: \n",
    "        # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "        wandb.log({\"episode number\": episode_num + 1})\n",
    "        wandb.log({\"episode timesteps\": episode_timesteps})\n",
    "        wandb.log({\"episode reward\": episode_reward})\n",
    "        wandb.log({\"total timesteps\": t + 1})\n",
    "        \n",
    "        \n",
    "        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "        # Reset environment\n",
    "        env_info = env.reset(train_mode=True)[brain_name] \n",
    "        state = env_info.vector_observations[0]\n",
    "        done = env_info.local_done[0]\n",
    "        scores.append(episode_reward)\n",
    "        scores_deque.append(episode_reward)  \n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num += 1\n",
    "        if episode_num % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode_num, np.mean(scores_deque)))\n",
    "            wandb.log({\"Average Reward over last 100 episodes\": np.mean(scores_deque)})\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.save(f\"./models/308_reacher_3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd_fresh",
   "language": "python",
   "name": "drlnd_fresh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
